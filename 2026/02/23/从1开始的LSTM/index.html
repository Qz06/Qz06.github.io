<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>从1开始的LSTM | Qz's Blog</title><meta name="author" content="Qz"><meta name="copyright" content="Qz"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="为什么是“从1开始”？因为RNN的逻辑实在太绕，而且正逐渐被Transformer取代，所以我直接调用了nn.LSTM，重点在于数据集处理和训练，预测函数 123456789101112131415161718192021222324252627282930import torchfrom torch import nnimport collectionsimport reclass Simple">
<meta property="og:type" content="article">
<meta property="og:title" content="从1开始的LSTM">
<meta property="og:url" content="https://www.cosmosh.ink/2026/02/23/%E4%BB%8E1%E5%BC%80%E5%A7%8B%E7%9A%84LSTM/index.html">
<meta property="og:site_name" content="Qz&#39;s Blog">
<meta property="og:description" content="为什么是“从1开始”？因为RNN的逻辑实在太绕，而且正逐渐被Transformer取代，所以我直接调用了nn.LSTM，重点在于数据集处理和训练，预测函数 123456789101112131415161718192021222324252627282930import torchfrom torch import nnimport collectionsimport reclass Simple">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://www.cosmosh.ink/img/avatar.png">
<meta property="article:published_time" content="2026-02-23T14:37:39.000Z">
<meta property="article:modified_time" content="2026-02-23T14:43:41.992Z">
<meta property="article:author" content="Qz">
<meta property="article:tag" content="Pytorch">
<meta property="article:tag" content="造轮子">
<meta property="article:tag" content="NLP">
<meta property="article:tag" content="LSTM">
<meta property="article:tag" content="RNN">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://www.cosmosh.ink/img/avatar.png"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "从1开始的LSTM",
  "url": "https://www.cosmosh.ink/2026/02/23/%E4%BB%8E1%E5%BC%80%E5%A7%8B%E7%9A%84LSTM/",
  "image": "https://www.cosmosh.ink/img/avatar.png",
  "datePublished": "2026-02-23T14:37:39.000Z",
  "dateModified": "2026-02-23T14:43:41.992Z",
  "author": [
    {
      "@type": "Person",
      "name": "Qz",
      "url": "https://www.cosmosh.ink/"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon.ico"><link rel="canonical" href="https://www.cosmosh.ink/2026/02/23/%E4%BB%8E1%E5%BC%80%E5%A7%8B%E7%9A%84LSTM/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"未找到符合您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":200,"highlightFullpage":false,"highlightMacStyle":true},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '从1开始的LSTM',
  isHighlightShrink: false,
  isToc: false,
  pageType: 'post'
}</script><meta name="generator" content="Hexo 7.3.0"></head><body><div id="web_bg" style="background-image: url(/img/bg.png);"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/avatar.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">5</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">13</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">9</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div><div class="menus_item"><a class="site-page" href="/gallery/"><i class="fa-fw fas fa-images"></i><span> 相册</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(/img/arc-bg.jpg);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">Qz's Blog</span></a><a class="nav-page-title" href="/"><span class="site-name">从1开始的LSTM</span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div><div class="menus_item"><a class="site-page" href="/gallery/"><i class="fa-fw fas fa-images"></i><span> 相册</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">从1开始的LSTM</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2026-02-23T14:37:39.000Z" title="发表于 2026-02-23 22:37:39">2026-02-23</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2026-02-23T14:43:41.992Z" title="更新于 2026-02-23 22:43:41">2026-02-23</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%8A%80%E6%9C%AF/">技术</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%8A%80%E6%9C%AF/AI/">AI</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%8A%80%E6%9C%AF/AI/NLP/">NLP</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%8A%80%E6%9C%AF/AI/NLP/RNN/">RNN</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><p><em>为什么是“从1开始”？因为RNN的逻辑实在太绕，而且正逐渐被Transformer取代，所以我直接调用了nn.LSTM，重点在于数据集处理和训练，预测函数</em></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleRNNModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab_size, num_hiddens</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.num_hiddens = num_hiddens</span><br><span class="line">        <span class="comment"># 1. Embedding 层：把字符 ID 转成稠密向量</span></span><br><span class="line">        <span class="variable language_">self</span>.embedding = nn.Embedding(vocab_size, num_hiddens)</span><br><span class="line">        <span class="comment"># 2. RNN 层：核心循环动力</span></span><br><span class="line">        <span class="variable language_">self</span>.rnn = nn.LSTM(num_hiddens, num_hiddens, batch_first=<span class="literal">True</span>,num_layers=<span class="number">2</span>)</span><br><span class="line">        <span class="comment"># 3. 输出层：把隐藏状态映射回词表大小，预测下一个字符</span></span><br><span class="line">        <span class="variable language_">self</span>.linear = nn.Linear(num_hiddens, vocab_size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X, state</span>):</span><br><span class="line">        <span class="comment"># X 形状: (batch_size, num_steps)</span></span><br><span class="line">        X = <span class="variable language_">self</span>.embedding(X) </span><br><span class="line">        <span class="comment"># output 形状: (batch_size, num_steps, num_hiddens)</span></span><br><span class="line">        output, state = <span class="variable language_">self</span>.rnn(X, state)</span><br><span class="line">        <span class="comment"># 把时间步维度展平，一次性过线性层加速计算</span></span><br><span class="line">        <span class="comment"># y 形状: (batch_size * num_steps, vocab_size)</span></span><br><span class="line">        y = <span class="variable language_">self</span>.linear(output.reshape(-<span class="number">1</span>, output.shape[-<span class="number">1</span>]))</span><br><span class="line">        <span class="keyword">return</span> y, state</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">begin_state</span>(<span class="params">self, batch_size, device</span>):</span><br><span class="line">        <span class="comment"># 初始化隐藏状态 H 为 0</span></span><br><span class="line">        <span class="keyword">return</span> (torch.zeros((<span class="number">2</span>, batch_size, <span class="variable language_">self</span>.num_hiddens), device=device),</span><br><span class="line">                torch.zeros((<span class="number">2</span>, batch_size, <span class="variable language_">self</span>.num_hiddens), device=device))</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">read_time_machine</span>():</span><br><span class="line">    <span class="comment"># 这里假设你已经有了 txt 文件，或者直接从网络下载</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;timemachine.txt&#x27;</span>, <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        lines = f.readlines()</span><br><span class="line">    <span class="keyword">return</span> [re.sub(<span class="string">&#x27;[^A-Za-z]+&#x27;</span>, <span class="string">&#x27; &#x27;</span>, line).strip().lower() <span class="keyword">for</span> line <span class="keyword">in</span> lines]</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Vocab</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,tokens</span>):</span><br><span class="line">        counter=collections.Counter(tokens)</span><br><span class="line">        <span class="variable language_">self</span>.idx_to_token=[<span class="string">&#x27;&lt;unk&gt;&#x27;</span>]+[token <span class="keyword">for</span> (token,freq) <span class="keyword">in</span> counter.items()]</span><br><span class="line">        <span class="variable language_">self</span>.token_to_idx=&#123;token:idx <span class="keyword">for</span> (idx,token) <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="variable language_">self</span>.idx_to_token)&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.idx_to_token)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, key</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(key,(<span class="built_in">list</span>,<span class="built_in">tuple</span>)):</span><br><span class="line">            <span class="keyword">return</span> <span class="variable language_">self</span>.token_to_idx.get(key,<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> [<span class="variable language_">self</span>.__getitem__(token) <span class="keyword">for</span> token <span class="keyword">in</span> key]</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">lines=read_time_machine()</span><br><span class="line">tokens=[token <span class="keyword">for</span> line <span class="keyword">in</span> lines <span class="keyword">for</span> token <span class="keyword">in</span> line]</span><br><span class="line">vocab=Vocab(tokens)</span><br><span class="line">corpus=vocab[tokens]</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset,DataLoader</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TMDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,corpus,num_steps</span>):</span><br><span class="line">        <span class="variable language_">self</span>.corpus=corpus</span><br><span class="line">        <span class="variable language_">self</span>.num_steps=num_steps</span><br><span class="line">        <span class="variable language_">self</span>.num_samples=<span class="built_in">len</span>(corpus)-num_steps</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.num_samples</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self,idx</span>):</span><br><span class="line">        X=torch.tensor(<span class="variable language_">self</span>.corpus[idx:idx+<span class="variable language_">self</span>.num_steps])</span><br><span class="line">        y=torch.tensor(<span class="variable language_">self</span>.corpus[idx+<span class="number">1</span>:idx+<span class="number">1</span>+<span class="variable language_">self</span>.num_steps])</span><br><span class="line">        <span class="keyword">return</span> X,y</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">batch_size=<span class="number">64</span></span><br><span class="line">num_steps=<span class="number">32</span></span><br><span class="line">lr=<span class="number">0.01</span></span><br><span class="line">epochs=<span class="number">20</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dataset=TMDataset(corpus,num_steps)</span><br><span class="line"></span><br><span class="line">train_loader=DataLoader(dataset,batch_size,shuffle=<span class="literal">True</span>,drop_last=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">net=SimpleRNNModel(vocab_size=<span class="built_in">len</span>(vocab),num_hiddens=<span class="number">256</span>)</span><br><span class="line">loss=nn.CrossEntropyLoss()</span><br><span class="line">optimizer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.1</span>, momentum=<span class="number">0.9</span>) </span><br><span class="line">scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)</span><br><span class="line">device=torch.device(<span class="string">&#x27;mps&#x27;</span>)</span><br><span class="line">net.to(device)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> e <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    net.train()</span><br><span class="line">    tloss=<span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> X,y <span class="keyword">in</span> train_loader:</span><br><span class="line">        X,y=X.to(device),y.to(device)</span><br><span class="line">        state=net.begin_state(batch_size,device)</span><br><span class="line">        y_pred,_=net(X,state)</span><br><span class="line">        l=loss(y_pred,y.reshape(-<span class="number">1</span>).long())</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        l.backward()</span><br><span class="line">        nn.utils.clip_grad_norm_(net.parameters(),max_norm=<span class="number">1</span>)</span><br><span class="line">        optimizer.step()</span><br><span class="line">        tloss+=l.item()</span><br><span class="line"></span><br><span class="line">    scheduler.step()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;epoch <span class="subst">&#123;e + <span class="number">1</span>&#125;</span>, loss <span class="subst">&#123;tloss / <span class="built_in">len</span>(train_loader):<span class="number">.4</span>f&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<pre><code>epoch 1, loss 1.6600
epoch 2, loss 1.1736
epoch 3, loss 0.9350
epoch 4, loss 0.7585
epoch 5, loss 0.6386
epoch 6, loss 0.5594
epoch 7, loss 0.5061
epoch 8, loss 0.4689
epoch 9, loss 0.4426
epoch 10, loss 0.4229
epoch 11, loss 0.4076
epoch 12, loss 0.3953
epoch 13, loss 0.3854
epoch 14, loss 0.3770
epoch 15, loss 0.3701
epoch 16, loss 0.3644
epoch 17, loss 0.3598
epoch 18, loss 0.3562
epoch 19, loss 0.3536
epoch 20, loss 0.3520
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">prefix,num_preds,net,vocab,device</span>):</span><br><span class="line">    outputs=[]</span><br><span class="line">    net.<span class="built_in">eval</span>()</span><br><span class="line">    outputs.append(vocab[prefix[<span class="number">0</span>]])</span><br><span class="line">    state=net.begin_state(<span class="number">1</span>,device)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_last_token</span>():</span><br><span class="line">        <span class="keyword">return</span> torch.tensor([outputs[-<span class="number">1</span>]],device=device,dtype=torch.long).reshape((<span class="number">1</span>,<span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#warmup</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,<span class="built_in">len</span>(prefix)):</span><br><span class="line">        _,state=net(get_last_token(),state)</span><br><span class="line">        outputs.append(vocab[prefix[i]])</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#predict</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(num_preds):</span><br><span class="line">        y_hat,state=net(get_last_token(),state)</span><br><span class="line">        outputs.append(y_hat.argmax(dim=<span class="number">1</span>).item())</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;&#x27;</span>.join([vocab.idx_to_token[i] <span class="keyword">for</span> i <span class="keyword">in</span> outputs])</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">predict(<span class="string">&#x27;the&#x27;</span>,<span class="number">200</span>,net,vocab,device)</span><br></pre></td></tr></table></figure>




<pre><code>&#39;the sun grow larger and dullerin the westward sky and the life of the old earth ebb away atlast more than thirty million years hence the huge red hot dome ofthe sun had come to obscure nearly a tenth par&#39;
</code></pre>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://www.cosmosh.ink">Qz</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://www.cosmosh.ink/2026/02/23/%E4%BB%8E1%E5%BC%80%E5%A7%8B%E7%9A%84LSTM/">https://www.cosmosh.ink/2026/02/23/%E4%BB%8E1%E5%BC%80%E5%A7%8B%E7%9A%84LSTM/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://www.cosmosh.ink" target="_blank">Qz's Blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Pytorch/">Pytorch</a><a class="post-meta__tags" href="/tags/%E9%80%A0%E8%BD%AE%E5%AD%90/">造轮子</a><a class="post-meta__tags" href="/tags/NLP/">NLP</a><a class="post-meta__tags" href="/tags/LSTM/">LSTM</a><a class="post-meta__tags" href="/tags/RNN/">RNN</a></div><div class="post-share"><div class="social-share" data-image="/img/avatar.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related full-width" href="/2026/02/23/%E4%BB%8E0%E5%BC%80%E5%A7%8B%E7%9A%84ResNet/" title="从0开始的ResNet"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">从0开始的ResNet</div></div><div class="info-2"><div class="info-item-1">123456789101112131415161718192021222324import torchimport torchvisionimport torchvision.transforms as transformsimport torch.nn as nntransform_train=transforms.Compose([    transforms.RandomCrop(32,padding=4),    transforms.RandomHorizontalFlip(),    transforms.ToTensor(),    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),])transform_test=transforms.Compose([    transforms.ToTensor(),    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994,...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2026/02/23/%E4%BB%8E0%E5%BC%80%E5%A7%8B%E7%9A%84Transformer/" title="从0开始的Transformer"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2026-02-23</div><div class="info-item-2">从0开始的Transformer</div></div><div class="info-2"><div class="info-item-1">基于pytorch，从底层实现transformer1234import torchfrom torch import nnimport mathimport re   1device=torch.device(&#x27;mps&#x27;)  读取数据集，这里使用”time machine” 12345def read_time_machine():    # 这里假设你已经有了 txt 文件，或者直接从网络下载    with open(&#x27;timemachine.txt&#x27;, &#x27;r&#x27;) as f:        lines = f.readlines()    return [re.sub(&#x27;[^A-Za-z]+&#x27;, &#x27; &#x27;, line).strip().lower() for line in...</div></div></div></a><a class="pagination-related" href="/2026/02/23/%E4%BB%8E0%E5%BC%80%E5%A7%8B%E7%9A%84ResNet/" title="从0开始的ResNet"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2026-02-23</div><div class="info-item-2">从0开始的ResNet</div></div><div class="info-2"><div class="info-item-1">123456789101112131415161718192021222324import torchimport torchvisionimport torchvision.transforms as transformsimport torch.nn as nntransform_train=transforms.Compose([    transforms.RandomCrop(32,padding=4),    transforms.RandomHorizontalFlip(),    transforms.ToTensor(),    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),])transform_test=transforms.Compose([    transforms.ToTensor(),    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994,...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/avatar.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">Qz</div><div class="author-info-description">Жизнь полюбить больше, чем смысл ее? — Непременно так, полюбить прежде логики...</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">5</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">13</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">9</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/Qz06"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/Qz06" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="/2791144859@qq.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2026/02/23/%E4%BB%8E1%E5%BC%80%E5%A7%8B%E7%9A%84LSTM/" title="从1开始的LSTM">从1开始的LSTM</a><time datetime="2026-02-23T14:37:39.000Z" title="发表于 2026-02-23 22:37:39">2026-02-23</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2026/02/23/%E4%BB%8E0%E5%BC%80%E5%A7%8B%E7%9A%84ResNet/" title="从0开始的ResNet">从0开始的ResNet</a><time datetime="2026-02-23T14:33:36.000Z" title="发表于 2026-02-23 22:33:36">2026-02-23</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2026/02/23/%E4%BB%8E0%E5%BC%80%E5%A7%8B%E7%9A%84Transformer/" title="从0开始的Transformer">从0开始的Transformer</a><time datetime="2026-02-23T06:04:31.000Z" title="发表于 2026-02-23 14:04:31">2026-02-23</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/02/23/BlogGuide/" title="BlogGuide">BlogGuide</a><time datetime="2025-02-22T16:24:38.000Z" title="发表于 2025-02-23 00:24:38">2025-02-23</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/02/22/hello-world/" title="Hello World">Hello World</a><time datetime="2025-02-22T10:23:56.907Z" title="发表于 2025-02-22 18:23:56">2025-02-22</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2026 By Qz</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 7.3.0</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.3.3</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"></div><canvas class="fireworks" mobile="false"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/fireworks.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="text-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/koharu.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":false},"log":false});</script></body></html>